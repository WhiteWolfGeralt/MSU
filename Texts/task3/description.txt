Здесь я попытался сделать предельно простое решение. Взят мультиязычный предобученный BERT
подходящего размера с huggingface, даже без поддержки русского языка(т.к. версии mini c
поддержкой русского языка там просто нет) и обучил на NEREL, взяв за основу достаточно простую
конфигурацию Trainer и максимально стандартные гиперпараметры. Также т.к. по выходу токенизатора
можно понять, закончилось слово на токене или еще нет(совпадает ли начало токена с концом предыдущего),
то были добавлены и простые фиксы окончаний слов(изредка помогает, так что почему бы халявно не повысить
чуть-чуть качество. Оценка качества на обучении также максимально тупая - потокенный F1 через
имплементацию от sklearn.